{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0b7758c2-9824-4b5a-9f0b-6b657cab79f1",
   "metadata": {
    "id": "0b7758c2-9824-4b5a-9f0b-6b657cab79f1"
   },
   "source": [
    "# Week 14: Colab Experiment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2001f926-2262-4c66-8abb-5eb2dd13766d",
   "metadata": {
    "id": "2001f926-2262-4c66-8abb-5eb2dd13766d"
   },
   "source": [
    "# I. Introduction\n",
    "In this exercise, we first train a transformer using the Wikitext-2 dataset and then use the model to generate new text with the length specified by the user.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7aa8f46-ff8b-47be-9bcc-12d5df8ec4d6",
   "metadata": {
    "id": "c7aa8f46-ff8b-47be-9bcc-12d5df8ec4d6"
   },
   "source": [
    "# II. Methods\n",
    "\n",
    "\n",
    "### 1. Library Imports\n",
    "The necessary libraries are imported to handle neural network operations, math calculations, and time tracking:\n",
    "- **torch**: For building and training the model.\n",
    "- **math**: For mathematical calculations.\n",
    "- **time**: For tracking training and evaluation time.\n",
    "\n",
    "### 2. Data Loading\n",
    "The data is processed using a custom `data.Corpus` class that handles text data. The data is then split into training, validation, and test datasets. The following function is used to prepare the datasets for training:\n",
    "- **batchify**: Organizes the dataset into batches of a specified size for efficient training.\n",
    "\n",
    "### 3. Model Construction\n",
    "The core model is based on the Transformer architecture, with the following components:\n",
    "- **PositionalEncoding**: A class to add positional information to the input sequences since Transformers don't inherently handle sequence order.\n",
    "- **TransformerModel**: A class derived from `nn.Transformer`, modified to handle embedding inputs and applying positional encoding.\n",
    "\n",
    "### 4. Training Functions\n",
    "Several functions are defined to handle training, evaluation, and batch preparation:\n",
    "- **get_batch**: Retrieves batches of data for training.\n",
    "- **train**: The training loop where the model receives data, computes the loss, performs backpropagation, and updates the model's parameters using gradient descent. Gradient clipping is applied to avoid large updates and stabilize training.\n",
    "- **evaluate**: Computes the loss on validation or test data to evaluate the performance of the trained model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4fb45f3e-9428-4353-a8cf-0ee4ae37cfa7",
   "metadata": {
    "id": "4fb45f3e-9428-4353-a8cf-0ee4ae37cfa7",
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "import time\n",
    "import math\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2a0ea711-14fe-4d85-892a-4f6c871bb81b",
   "metadata": {
    "id": "2a0ea711-14fe-4d85-892a-4f6c871bb81b",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Uncomment one of the following that works for you.\n",
    "\n",
    "# device = torch.device(\"cuda\")\n",
    "device = torch.device(\"mps\")\n",
    "# device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "024af44e-ee91-4d32-aa01-feb09f9dddcf",
   "metadata": {
    "id": "024af44e-ee91-4d32-aa01-feb09f9dddcf",
    "tags": []
   },
   "outputs": [],
   "source": [
    "batch_size = 20\n",
    "\n",
    "emsize = 200 # size of word embeddings\n",
    "nhead = 2\n",
    "nhid = 200\n",
    "nlayers = 2\n",
    "dropout = 0.2\n",
    "lr = 20 # initial learning rate\n",
    "epochs=10 # upper epoch limit\n",
    "\n",
    "bptt=35 #sequence length\n",
    "clip=0.25 #gradient clipping\n",
    "log_interval=200 # report interval\n",
    "\n",
    "save='model.pt' #path to save the final model\n",
    "\n",
    "# Set the random seed manually for reproducibility.\n",
    "torch.manual_seed(0)\n",
    "\n",
    "eval_batch_size = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e435b94-c544-447b-8213-21f4c64fafef",
   "metadata": {
    "id": "5e435b94-c544-447b-8213-21f4c64fafef"
   },
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "pHGuUFWZ7mpw",
   "metadata": {
    "id": "pHGuUFWZ7mpw"
   },
   "outputs": [],
   "source": [
    "\n",
    "import sys\n",
    "sys.path.append('./') # Change to your own path\n",
    "import data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "382d8c86-7df9-4653-af82-b2515861e362",
   "metadata": {
    "id": "382d8c86-7df9-4653-af82-b2515861e362",
    "tags": []
   },
   "outputs": [],
   "source": [
    "corpus = data.Corpus('./data/wikitext-2')\n",
    "\n",
    "def batchify(data, bsz):\n",
    "    nbatch = data.size(0) // bsz\n",
    "    data = data.narrow(0, 0, nbatch * bsz)\n",
    "    data = data.view(bsz, -1).t().contiguous()\n",
    "    return data.to(device)\n",
    "\n",
    "train_data = batchify(corpus.train, batch_size)\n",
    "val_data = batchify(corpus.valid, eval_batch_size)\n",
    "test_data = batchify(corpus.test, eval_batch_size)\n",
    "ntokens = len(corpus.dictionary)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f968342b-ee5f-41b7-aa97-6b7eaefc3eda",
   "metadata": {
    "id": "f968342b-ee5f-41b7-aa97-6b7eaefc3eda"
   },
   "source": [
    "## Build the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a036d973-53ba-4f4b-abfa-188da6008ab3",
   "metadata": {
    "id": "a036d973-53ba-4f4b-abfa-188da6008ab3"
   },
   "outputs": [],
   "source": [
    "# Define positional encoding used in the transformer model\n",
    "\n",
    "#################################################################################################\n",
    "# [TODO]: Build a positional encoding function that can be used in the TransformerModel below\n",
    "#################################################################################################\n",
    "\n",
    "class PositionalEncoding(nn.Module):\n",
    "\n",
    "    def __init__(self, d_model, dropout=0.1, max_len=5000):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        \n",
    "        # Initialize dropout layer\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "        # Create a long enough position encoding matrix\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        \n",
    "        # Compute the positional encoding values\n",
    "        for pos in range(max_len):\n",
    "            for i in range(0, d_model, 2):\n",
    "                # Apply the sine function to even indices\n",
    "                pe[pos, i] = math.sin(pos / (10000 ** (i / d_model)))\n",
    "                # Apply the cosine function to odd indices\n",
    "                if i + 1 < d_model:\n",
    "                    pe[pos, i + 1] = math.cos(pos / (10000 ** ((i + 1) / d_model)))\n",
    "        \n",
    "        # Add a batch dimension to pe, then register as a buffer to not be updated by gradients\n",
    "        pe = pe.unsqueeze(0)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Add the positional encoding to the input tensor\n",
    "        x = x + self.pe[:, :x.size(1)].detach()  # detaching to prevent gradients from updating it\n",
    "        return self.dropout(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7464215f-d023-4407-aca3-9dffd6753d53",
   "metadata": {},
   "source": [
    "This code implements **Positional Encoding**, which is used in the **Transformer** model to assign a unique representation to each position in a sequence, allowing the model to capture the order of elements in the sequence.\n",
    "\n",
    "- `pe`: A zero matrix of size `(max_len, d_model)`, representing the encoding for each position up to the maximum length.\n",
    "- For each position `pos` (from 0 to `max_len-1`), the position encoding is calculated using a position formula.\n",
    "- For even-indexed dimensions (`i`), the **sine function** (`sin`) is used.\n",
    "- For odd-indexed dimensions (`i+1`), the **cosine function** (`cos`) is used.\n",
    "- These formulas ensure that each position has a unique encoding, and the encoding varies based on the position. By using `10000` as a base, the changes in higher dimensions are smoother.\n",
    "- `pe.unsqueeze(0)`: Adds a batch dimension to the positional encoding matrix, changing its shape to `(1, max_len, d_model)`, so it can match the shape of the input data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "724cbe76-35db-422e-9de9-21a2ff4f5798",
   "metadata": {
    "id": "724cbe76-35db-422e-9de9-21a2ff4f5798"
   },
   "outputs": [],
   "source": [
    "# Define the transformer model\n",
    "# Define the TransformerModel class inheriting from nn.Transformer\n",
    "class TransformerModel(nn.Transformer):\n",
    "\n",
    "    #Initialize the Transformer model.\n",
    "    \n",
    "    #Parameters:\n",
    "    #- ntoken: The number of tokens in the vocabulary (input/output size).\n",
    "    #- ninp: Dimensionality of the input embeddings.\n",
    "    #- nhead: Number of attention heads in the multi-head attention mechanism.\n",
    "    #- nhid: Dimensionality of the feedforward network in the transformer layers.\n",
    "    #- nlayers: Number of encoder layers in the transformer.\n",
    "    #- dropout: Dropout rate for regularization.\n",
    "\n",
    "    def __init__(self, ntoken, ninp, nhead, nhid, nlayers, dropout=0.5):\n",
    "        # Call the parent class constructor with the appropriate arguments\n",
    "        super(TransformerModel, self).__init__(d_model=ninp, nhead=nhead, dim_feedforward=nhid, num_encoder_layers=nlayers)\n",
    "         # Model type for reference\n",
    "        self.model_type = 'Transformer'\n",
    "        # Placeholder for the source mask\n",
    "        self.src_mask = None\n",
    "        # Positional encoding to inject position information into the input embeddings\n",
    "        self.pos_encoder = PositionalEncoding(ninp, dropout) # This is what you had constructed above\n",
    "        # Embedding layer for input tokens\n",
    "\n",
    "        self.input_emb = nn.Embedding(ntoken, ninp)\n",
    "        self.ninp = ninp# Store the input embedding dimension\n",
    "        # Linear layer to decode transformer outputs into vocabulary logits\n",
    "        self.decoder = nn.Linear(ninp, ntoken)\n",
    "        # Initialize weights for the model\n",
    "\n",
    "        self.init_weights()\n",
    "        \n",
    "        #Generate a square mask for the sequence to ensure that a token \n",
    "        #only attends to previous tokens (used in language modeling).\n",
    "        \n",
    "        #Parameters:\n",
    "        #- sz: Size of the mask (sequence length).\n",
    "        \n",
    "        #Returns:\n",
    "        #- A lower triangular log-transformed mask.\n",
    " \n",
    "    def _generate_square_subsequent_mask(self, sz):\n",
    "        return torch.log(torch.tril(torch.ones(sz,sz)))\n",
    "\n",
    "    def init_weights(self):\n",
    "        initrange = 0.1\n",
    "        nn.init.uniform_(self.input_emb.weight, -initrange, initrange)\n",
    "        nn.init.zeros_(self.decoder.bias)\n",
    "        nn.init.uniform_(self.decoder.weight, -initrange, initrange)\n",
    "\n",
    "    def forward(self, src, has_mask=True):\n",
    "        \"\"\"\n",
    "        Forward pass for the transformer model.\n",
    "        \n",
    "        Parameters:\n",
    "        - src: Input sequence of token indices.\n",
    "        - has_mask: Flag to determine if a mask should be applied.\n",
    "        \n",
    "        Returns:\n",
    "        - Log-softmax probabilities over the vocabulary for each token.\n",
    "        \"\"\"\n",
    "        if has_mask:\n",
    "            device = src.device\n",
    "            if self.src_mask is None or self.src_mask.size(0) != len(src):\n",
    "                mask = self._generate_square_subsequent_mask(len(src)).to(device)\n",
    "                self.src_mask = mask\n",
    "        else:\n",
    "            self.src_mask = None\n",
    "\n",
    "        src = self.input_emb(src) * math.sqrt(self.ninp)\n",
    "        src = self.pos_encoder(src)\n",
    "        output = self.encoder(src, mask=self.src_mask)\n",
    "        output = self.decoder(output)\n",
    "        return F.log_softmax(output, dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8c8a12a5-50b2-4f78-9f2a-c98e3a746e30",
   "metadata": {
    "id": "8c8a12a5-50b2-4f78-9f2a-c98e3a746e30"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.12/site-packages/torch/nn/modules/transformer.py:379: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "model = TransformerModel(ntokens, emsize, nhead, nhid, nlayers, dropout).to(device)\n",
    "criterion = nn.NLLLoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a7378de-eeac-445c-8f15-768681785fcd",
   "metadata": {
    "id": "6a7378de-eeac-445c-8f15-768681785fcd"
   },
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1facb91b-6a0d-42b2-9cec-600fc15f27d6",
   "metadata": {
    "id": "1facb91b-6a0d-42b2-9cec-600fc15f27d6",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch   1 |   200/ 2983 batches | lr 20.00 | ms/batch 94.91 | loss 16.50 | ppl 14676497.04\n",
      "| epoch   1 |   400/ 2983 batches | lr 20.00 | ms/batch 96.22 | loss 12.49 | ppl 265771.89\n",
      "| epoch   1 |   600/ 2983 batches | lr 20.00 | ms/batch 94.79 | loss 10.49 | ppl 35803.16\n",
      "| epoch   1 |   800/ 2983 batches | lr 20.00 | ms/batch 94.32 | loss  9.71 | ppl 16466.20\n",
      "| epoch   1 |  1000/ 2983 batches | lr 20.00 | ms/batch 94.44 | loss  9.27 | ppl 10619.17\n",
      "| epoch   1 |  1200/ 2983 batches | lr 20.00 | ms/batch 95.20 | loss  9.05 | ppl  8528.49\n",
      "| epoch   1 |  1400/ 2983 batches | lr 20.00 | ms/batch 93.66 | loss  8.76 | ppl  6354.50\n",
      "| epoch   1 |  1600/ 2983 batches | lr 20.00 | ms/batch 80.48 | loss  8.96 | ppl  7816.05\n",
      "| epoch   1 |  1800/ 2983 batches | lr 20.00 | ms/batch 80.48 | loss  8.77 | ppl  6465.86\n",
      "| epoch   1 |  2000/ 2983 batches | lr 20.00 | ms/batch 80.40 | loss  8.74 | ppl  6226.95\n",
      "| epoch   1 |  2200/ 2983 batches | lr 20.00 | ms/batch 79.62 | loss  8.63 | ppl  5614.36\n",
      "| epoch   1 |  2400/ 2983 batches | lr 20.00 | ms/batch 80.00 | loss  8.64 | ppl  5649.21\n",
      "| epoch   1 |  2600/ 2983 batches | lr 20.00 | ms/batch 80.56 | loss  8.67 | ppl  5845.27\n",
      "| epoch   1 |  2800/ 2983 batches | lr 20.00 | ms/batch 91.49 | loss  8.52 | ppl  5037.88\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   1 | time: 279.55s | valid loss  7.96 | valid ppl  2869.36\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch   2 |   200/ 2983 batches | lr 20.00 | ms/batch 95.29 | loss  8.47 | ppl  4752.90\n",
      "| epoch   2 |   400/ 2983 batches | lr 20.00 | ms/batch 93.37 | loss  8.52 | ppl  5021.80\n",
      "| epoch   2 |   600/ 2983 batches | lr 20.00 | ms/batch 95.78 | loss  8.64 | ppl  5659.05\n",
      "| epoch   2 |   800/ 2983 batches | lr 20.00 | ms/batch 81.50 | loss  8.46 | ppl  4713.60\n",
      "| epoch   2 |  1000/ 2983 batches | lr 20.00 | ms/batch 88.37 | loss  8.51 | ppl  4949.51\n",
      "| epoch   2 |  1200/ 2983 batches | lr 20.00 | ms/batch 91.85 | loss  8.53 | ppl  5077.19\n",
      "| epoch   2 |  1400/ 2983 batches | lr 20.00 | ms/batch 90.10 | loss  8.41 | ppl  4505.56\n",
      "| epoch   2 |  1600/ 2983 batches | lr 20.00 | ms/batch 81.27 | loss  8.55 | ppl  5169.32\n",
      "| epoch   2 |  1800/ 2983 batches | lr 20.00 | ms/batch 92.20 | loss  8.41 | ppl  4503.04\n",
      "| epoch   2 |  2000/ 2983 batches | lr 20.00 | ms/batch 89.73 | loss  8.43 | ppl  4587.65\n",
      "| epoch   2 |  2200/ 2983 batches | lr 20.00 | ms/batch 86.26 | loss  8.47 | ppl  4765.88\n",
      "| epoch   2 |  2400/ 2983 batches | lr 20.00 | ms/batch 94.06 | loss  8.35 | ppl  4247.91\n",
      "| epoch   2 |  2600/ 2983 batches | lr 20.00 | ms/batch 83.20 | loss  8.40 | ppl  4435.56\n",
      "| epoch   2 |  2800/ 2983 batches | lr 20.00 | ms/batch 80.05 | loss  8.35 | ppl  4219.16\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   2 | time: 277.89s | valid loss  9.65 | valid ppl 15558.45\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch   3 |   200/ 2983 batches | lr 5.00 | ms/batch 95.33 | loss  7.14 | ppl  1258.43\n",
      "| epoch   3 |   400/ 2983 batches | lr 5.00 | ms/batch 95.83 | loss  7.07 | ppl  1172.25\n",
      "| epoch   3 |   600/ 2983 batches | lr 5.00 | ms/batch 92.44 | loss  7.06 | ppl  1161.41\n",
      "| epoch   3 |   800/ 2983 batches | lr 5.00 | ms/batch 92.15 | loss  7.06 | ppl  1164.53\n",
      "| epoch   3 |  1000/ 2983 batches | lr 5.00 | ms/batch 95.06 | loss  7.08 | ppl  1182.73\n",
      "| epoch   3 |  1200/ 2983 batches | lr 5.00 | ms/batch 94.78 | loss  7.08 | ppl  1193.84\n",
      "| epoch   3 |  1400/ 2983 batches | lr 5.00 | ms/batch 92.74 | loss  7.06 | ppl  1163.33\n",
      "| epoch   3 |  1600/ 2983 batches | lr 5.00 | ms/batch 89.07 | loss  7.08 | ppl  1182.95\n",
      "| epoch   3 |  1800/ 2983 batches | lr 5.00 | ms/batch 92.55 | loss  7.05 | ppl  1149.35\n",
      "| epoch   3 |  2000/ 2983 batches | lr 5.00 | ms/batch 81.44 | loss  7.07 | ppl  1175.97\n",
      "| epoch   3 |  2200/ 2983 batches | lr 5.00 | ms/batch 80.03 | loss  7.07 | ppl  1180.60\n",
      "| epoch   3 |  2400/ 2983 batches | lr 5.00 | ms/batch 80.56 | loss  7.04 | ppl  1141.18\n",
      "| epoch   3 |  2600/ 2983 batches | lr 5.00 | ms/batch 81.56 | loss  7.07 | ppl  1171.22\n",
      "| epoch   3 |  2800/ 2983 batches | lr 5.00 | ms/batch 89.34 | loss  7.03 | ppl  1131.52\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   3 | time: 282.25s | valid loss  6.98 | valid ppl  1073.69\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch   4 |   200/ 2983 batches | lr 5.00 | ms/batch 93.21 | loss  7.06 | ppl  1164.21\n",
      "| epoch   4 |   400/ 2983 batches | lr 5.00 | ms/batch 85.26 | loss  7.03 | ppl  1131.02\n",
      "| epoch   4 |   600/ 2983 batches | lr 5.00 | ms/batch 95.09 | loss  7.02 | ppl  1114.05\n",
      "| epoch   4 |   800/ 2983 batches | lr 5.00 | ms/batch 86.22 | loss  7.03 | ppl  1124.96\n",
      "| epoch   4 |  1000/ 2983 batches | lr 5.00 | ms/batch 79.66 | loss  7.05 | ppl  1148.07\n",
      "| epoch   4 |  1200/ 2983 batches | lr 5.00 | ms/batch 79.71 | loss  7.06 | ppl  1158.90\n",
      "| epoch   4 |  1400/ 2983 batches | lr 5.00 | ms/batch 79.79 | loss  7.03 | ppl  1129.07\n",
      "| epoch   4 |  1600/ 2983 batches | lr 5.00 | ms/batch 79.76 | loss  7.04 | ppl  1142.56\n",
      "| epoch   4 |  1800/ 2983 batches | lr 5.00 | ms/batch 79.94 | loss  7.02 | ppl  1115.37\n",
      "| epoch   4 |  2000/ 2983 batches | lr 5.00 | ms/batch 79.69 | loss  7.04 | ppl  1142.86\n",
      "| epoch   4 |  2200/ 2983 batches | lr 5.00 | ms/batch 79.74 | loss  7.04 | ppl  1142.50\n",
      "| epoch   4 |  2400/ 2983 batches | lr 5.00 | ms/batch 79.80 | loss  7.00 | ppl  1101.11\n",
      "| epoch   4 |  2600/ 2983 batches | lr 5.00 | ms/batch 79.73 | loss  7.03 | ppl  1129.03\n",
      "| epoch   4 |  2800/ 2983 batches | lr 5.00 | ms/batch 79.96 | loss  7.00 | ppl  1091.86\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   4 | time: 260.95s | valid loss  7.06 | valid ppl  1160.45\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch   5 |   200/ 2983 batches | lr 1.25 | ms/batch 85.07 | loss  6.98 | ppl  1075.52\n",
      "| epoch   5 |   400/ 2983 batches | lr 1.25 | ms/batch 79.61 | loss  6.95 | ppl  1047.90\n",
      "| epoch   5 |   600/ 2983 batches | lr 1.25 | ms/batch 80.24 | loss  6.93 | ppl  1025.69\n",
      "| epoch   5 |   800/ 2983 batches | lr 1.25 | ms/batch 85.58 | loss  6.95 | ppl  1042.65\n",
      "| epoch   5 |  1000/ 2983 batches | lr 1.25 | ms/batch 94.90 | loss  6.97 | ppl  1064.36\n",
      "| epoch   5 |  1200/ 2983 batches | lr 1.25 | ms/batch 94.27 | loss  6.98 | ppl  1079.41\n",
      "| epoch   5 |  1400/ 2983 batches | lr 1.25 | ms/batch 94.82 | loss  6.96 | ppl  1053.36\n",
      "| epoch   5 |  1600/ 2983 batches | lr 1.25 | ms/batch 94.66 | loss  6.97 | ppl  1066.74\n",
      "| epoch   5 |  1800/ 2983 batches | lr 1.25 | ms/batch 91.75 | loss  6.94 | ppl  1037.48\n",
      "| epoch   5 |  2000/ 2983 batches | lr 1.25 | ms/batch 81.10 | loss  6.97 | ppl  1065.51\n",
      "| epoch   5 |  2200/ 2983 batches | lr 1.25 | ms/batch 79.71 | loss  6.96 | ppl  1058.08\n",
      "| epoch   5 |  2400/ 2983 batches | lr 1.25 | ms/batch 79.83 | loss  6.93 | ppl  1018.40\n",
      "| epoch   5 |  2600/ 2983 batches | lr 1.25 | ms/batch 88.58 | loss  6.95 | ppl  1047.37\n",
      "| epoch   5 |  2800/ 2983 batches | lr 1.25 | ms/batch 83.63 | loss  6.92 | ppl  1011.24\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   5 | time: 273.81s | valid loss  7.00 | valid ppl  1098.85\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch   6 |   200/ 2983 batches | lr 0.31 | ms/batch 95.42 | loss  7.01 | ppl  1109.83\n",
      "| epoch   6 |   400/ 2983 batches | lr 0.31 | ms/batch 93.21 | loss  6.98 | ppl  1079.36\n",
      "| epoch   6 |   600/ 2983 batches | lr 0.31 | ms/batch 79.66 | loss  6.97 | ppl  1067.70\n",
      "| epoch   6 |   800/ 2983 batches | lr 0.31 | ms/batch 79.77 | loss  6.98 | ppl  1071.45\n",
      "| epoch   6 |  1000/ 2983 batches | lr 0.31 | ms/batch 80.61 | loss  7.00 | ppl  1098.07\n",
      "| epoch   6 |  1200/ 2983 batches | lr 0.31 | ms/batch 87.98 | loss  7.01 | ppl  1108.88\n",
      "| epoch   6 |  1400/ 2983 batches | lr 0.31 | ms/batch 96.50 | loss  6.99 | ppl  1086.63\n",
      "| epoch   6 |  1600/ 2983 batches | lr 0.31 | ms/batch 87.42 | loss  7.00 | ppl  1098.70\n",
      "| epoch   6 |  1800/ 2983 batches | lr 0.31 | ms/batch 80.35 | loss  6.97 | ppl  1064.11\n",
      "| epoch   6 |  2000/ 2983 batches | lr 0.31 | ms/batch 86.00 | loss  7.00 | ppl  1094.11\n",
      "| epoch   6 |  2200/ 2983 batches | lr 0.31 | ms/batch 95.51 | loss  7.00 | ppl  1096.17\n",
      "| epoch   6 |  2400/ 2983 batches | lr 0.31 | ms/batch 82.53 | loss  6.98 | ppl  1071.15\n",
      "| epoch   6 |  2600/ 2983 batches | lr 0.31 | ms/batch 80.10 | loss  6.99 | ppl  1081.60\n",
      "| epoch   6 |  2800/ 2983 batches | lr 0.31 | ms/batch 80.66 | loss  6.96 | ppl  1055.72\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   6 | time: 270.75s | valid loss  7.00 | valid ppl  1092.24\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch   7 |   200/ 2983 batches | lr 0.08 | ms/batch 94.85 | loss  7.10 | ppl  1215.39\n",
      "| epoch   7 |   400/ 2983 batches | lr 0.08 | ms/batch 94.70 | loss  7.07 | ppl  1176.27\n",
      "| epoch   7 |   600/ 2983 batches | lr 0.08 | ms/batch 95.43 | loss  7.06 | ppl  1158.91\n",
      "| epoch   7 |   800/ 2983 batches | lr 0.08 | ms/batch 81.39 | loss  7.05 | ppl  1152.96\n",
      "| epoch   7 |  1000/ 2983 batches | lr 0.08 | ms/batch 89.50 | loss  7.06 | ppl  1163.05\n",
      "| epoch   7 |  1200/ 2983 batches | lr 0.08 | ms/batch 94.23 | loss  7.08 | ppl  1185.83\n",
      "| epoch   7 |  1400/ 2983 batches | lr 0.08 | ms/batch 95.04 | loss  7.06 | ppl  1163.58\n",
      "| epoch   7 |  1600/ 2983 batches | lr 0.08 | ms/batch 83.63 | loss  7.08 | ppl  1184.91\n",
      "| epoch   7 |  1800/ 2983 batches | lr 0.08 | ms/batch 80.70 | loss  7.04 | ppl  1139.13\n",
      "| epoch   7 |  2000/ 2983 batches | lr 0.08 | ms/batch 92.74 | loss  7.06 | ppl  1159.60\n",
      "| epoch   7 |  2200/ 2983 batches | lr 0.08 | ms/batch 87.90 | loss  7.08 | ppl  1183.40\n",
      "| epoch   7 |  2400/ 2983 batches | lr 0.08 | ms/batch 81.74 | loss  7.06 | ppl  1161.40\n",
      "| epoch   7 |  2600/ 2983 batches | lr 0.08 | ms/batch 82.52 | loss  7.07 | ppl  1170.54\n",
      "| epoch   7 |  2800/ 2983 batches | lr 0.08 | ms/batch 80.50 | loss  7.02 | ppl  1117.32\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   7 | time: 276.23s | valid loss  6.91 | valid ppl   999.14\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch   8 |   200/ 2983 batches | lr 0.08 | ms/batch 86.09 | loss  7.10 | ppl  1209.37\n",
      "| epoch   8 |   400/ 2983 batches | lr 0.08 | ms/batch 81.31 | loss  7.06 | ppl  1166.88\n",
      "| epoch   8 |   600/ 2983 batches | lr 0.08 | ms/batch 91.19 | loss  7.05 | ppl  1157.44\n",
      "| epoch   8 |   800/ 2983 batches | lr 0.08 | ms/batch 90.02 | loss  7.05 | ppl  1153.33\n",
      "| epoch   8 |  1000/ 2983 batches | lr 0.08 | ms/batch 79.68 | loss  7.06 | ppl  1158.81\n",
      "| epoch   8 |  1200/ 2983 batches | lr 0.08 | ms/batch 80.80 | loss  7.08 | ppl  1184.57\n",
      "| epoch   8 |  1400/ 2983 batches | lr 0.08 | ms/batch 80.48 | loss  7.06 | ppl  1161.32\n",
      "| epoch   8 |  1600/ 2983 batches | lr 0.08 | ms/batch 80.37 | loss  7.08 | ppl  1183.73\n",
      "| epoch   8 |  1800/ 2983 batches | lr 0.08 | ms/batch 80.87 | loss  7.04 | ppl  1135.85\n",
      "| epoch   8 |  2000/ 2983 batches | lr 0.08 | ms/batch 81.97 | loss  7.05 | ppl  1154.99\n",
      "| epoch   8 |  2200/ 2983 batches | lr 0.08 | ms/batch 87.66 | loss  7.07 | ppl  1179.36\n",
      "| epoch   8 |  2400/ 2983 batches | lr 0.08 | ms/batch 88.73 | loss  7.05 | ppl  1158.42\n",
      "| epoch   8 |  2600/ 2983 batches | lr 0.08 | ms/batch 86.18 | loss  7.06 | ppl  1167.99\n",
      "| epoch   8 |  2800/ 2983 batches | lr 0.08 | ms/batch 80.61 | loss  7.02 | ppl  1117.90\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   8 | time: 264.86s | valid loss  6.91 | valid ppl   999.49\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch   9 |   200/ 2983 batches | lr 0.02 | ms/batch 88.18 | loss  7.13 | ppl  1244.15\n",
      "| epoch   9 |   400/ 2983 batches | lr 0.02 | ms/batch 94.76 | loss  7.12 | ppl  1236.36\n",
      "| epoch   9 |   600/ 2983 batches | lr 0.02 | ms/batch 82.66 | loss  7.09 | ppl  1204.51\n",
      "| epoch   9 |   800/ 2983 batches | lr 0.02 | ms/batch 80.42 | loss  7.11 | ppl  1226.31\n",
      "| epoch   9 |  1000/ 2983 batches | lr 0.02 | ms/batch 80.07 | loss  7.10 | ppl  1217.10\n",
      "| epoch   9 |  1200/ 2983 batches | lr 0.02 | ms/batch 79.93 | loss  7.11 | ppl  1221.68\n",
      "| epoch   9 |  1400/ 2983 batches | lr 0.02 | ms/batch 79.77 | loss  7.08 | ppl  1189.87\n",
      "| epoch   9 |  1600/ 2983 batches | lr 0.02 | ms/batch 79.82 | loss  7.10 | ppl  1214.80\n",
      "| epoch   9 |  1800/ 2983 batches | lr 0.02 | ms/batch 79.88 | loss  7.07 | ppl  1178.04\n",
      "| epoch   9 |  2000/ 2983 batches | lr 0.02 | ms/batch 87.99 | loss  7.09 | ppl  1203.62\n",
      "| epoch   9 |  2200/ 2983 batches | lr 0.02 | ms/batch 81.74 | loss  7.08 | ppl  1187.95\n",
      "| epoch   9 |  2400/ 2983 batches | lr 0.02 | ms/batch 92.79 | loss  7.04 | ppl  1147.04\n",
      "| epoch   9 |  2600/ 2983 batches | lr 0.02 | ms/batch 81.67 | loss  7.07 | ppl  1175.95\n",
      "| epoch   9 |  2800/ 2983 batches | lr 0.02 | ms/batch 81.45 | loss  7.02 | ppl  1123.92\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   9 | time: 263.83s | valid loss  6.88 | valid ppl   971.71\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  10 |   200/ 2983 batches | lr 0.02 | ms/batch 81.22 | loss  7.11 | ppl  1225.16\n",
      "| epoch  10 |   400/ 2983 batches | lr 0.02 | ms/batch 79.72 | loss  7.08 | ppl  1193.05\n",
      "| epoch  10 |   600/ 2983 batches | lr 0.02 | ms/batch 79.95 | loss  7.08 | ppl  1185.27\n",
      "| epoch  10 |   800/ 2983 batches | lr 0.02 | ms/batch 79.83 | loss  7.09 | ppl  1201.56\n",
      "| epoch  10 |  1000/ 2983 batches | lr 0.02 | ms/batch 79.81 | loss  7.09 | ppl  1196.03\n",
      "| epoch  10 |  1200/ 2983 batches | lr 0.02 | ms/batch 79.73 | loss  7.10 | ppl  1206.50\n",
      "| epoch  10 |  1400/ 2983 batches | lr 0.02 | ms/batch 80.65 | loss  7.07 | ppl  1181.89\n",
      "| epoch  10 |  1600/ 2983 batches | lr 0.02 | ms/batch 79.69 | loss  7.09 | ppl  1201.86\n",
      "| epoch  10 |  1800/ 2983 batches | lr 0.02 | ms/batch 80.44 | loss  7.06 | ppl  1169.81\n",
      "| epoch  10 |  2000/ 2983 batches | lr 0.02 | ms/batch 83.44 | loss  7.09 | ppl  1197.56\n",
      "| epoch  10 |  2200/ 2983 batches | lr 0.02 | ms/batch 92.04 | loss  7.08 | ppl  1187.94\n",
      "| epoch  10 |  2400/ 2983 batches | lr 0.02 | ms/batch 93.10 | loss  7.05 | ppl  1152.61\n",
      "| epoch  10 |  2600/ 2983 batches | lr 0.02 | ms/batch 94.14 | loss  7.08 | ppl  1185.77\n",
      "| epoch  10 |  2800/ 2983 batches | lr 0.02 | ms/batch 94.85 | loss  7.03 | ppl  1132.40\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  10 | time: 266.16s | valid loss  6.88 | valid ppl   968.07\n",
      "-----------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/4f/vh2dsmy92gdckq5_0qw04whr0000gn/T/ipykernel_57919/2565661040.py:85: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model = torch.load(f)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=========================================================================================\n",
      "| End of training | test loss  6.81 | test ppl   911.16\n",
      "=========================================================================================\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "def get_batch(source, i):\n",
    "    \"\"\"\n",
    "    Fetches a batch of data and its corresponding target from the dataset.\n",
    "\n",
    "    Parameters:\n",
    "    - source: The dataset (tensor) to fetch data from.\n",
    "    - i: The starting index of the batch.\n",
    "\n",
    "    Returns:\n",
    "    - data: Input sequence (length: seq_len).\n",
    "    - target: Target sequence (length: seq_len, flattened for loss calculation).\n",
    "    \"\"\"\n",
    "    seq_len = min(bptt, len(source) - 1 - i)\n",
    "    data = source[i:i+seq_len]\n",
    "    target = source[i+1:i+1+seq_len].view(-1)\n",
    "    return data, target\n",
    "\n",
    "\n",
    "def evaluate(data_source):\n",
    "    \"\"\"\n",
    "    Evaluates the model on the given dataset.\n",
    "\n",
    "    Parameters:\n",
    "    - data_source: Dataset to evaluate on (validation or test data).\n",
    "\n",
    "    Returns:\n",
    "    - Average loss per token across the dataset.\n",
    "    \"\"\"\n",
    "    # Turn on evaluation mode which disables dropout.\n",
    "    model.eval()\n",
    "    total_loss = 0.\n",
    "    ntokens = len(corpus.dictionary)\n",
    "    with torch.no_grad():\n",
    "        for i in range(0, data_source.size(0) - 1, bptt):\n",
    "            data, targets = get_batch(data_source, i)\n",
    "            output = model(data)\n",
    "            output = output.view(-1, ntokens)\n",
    "\n",
    "            total_loss += len(data) * criterion(output, targets).item()\n",
    "    return total_loss / (len(data_source) - 1)\n",
    "\n",
    "\n",
    "def train():\n",
    "    \"\"\"\n",
    "    Trains the model for one epoch on the training data.\n",
    "\n",
    "    Returns:\n",
    "    - None\n",
    "    \"\"\"\n",
    "    # Turn on training mode which enables dropout.\n",
    "    model.train()\n",
    "    total_loss = 0.\n",
    "    start_time = time.time()\n",
    "    ntokens = len(corpus.dictionary)\n",
    "    for batch, i in enumerate(range(0, train_data.size(0) - 1, bptt)):\n",
    "        data, targets = get_batch(train_data, i)\n",
    "        # Starting each batch, we detach the hidden state from how it was previously produced.\n",
    "        # If we didn't, the model would try backpropagating all the way to start of the dataset.\n",
    "        model.zero_grad()\n",
    "        output = model(data)\n",
    "        output = output.view(-1, ntokens)\n",
    "        loss = criterion(output, targets)\n",
    "        loss.backward()\n",
    "\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
    "        for p in model.parameters():\n",
    "            p.data.add_(p.grad, alpha=-lr)\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "        if batch % log_interval == 0 and batch > 0:\n",
    "            cur_loss = total_loss / log_interval\n",
    "            elapsed = time.time() - start_time\n",
    "            print('| epoch {:3d} | {:5d}/{:5d} batches | lr {:02.2f} | ms/batch {:5.2f} | '\n",
    "                    'loss {:5.2f} | ppl {:8.2f}'.format(\n",
    "                epoch, batch, len(train_data) // bptt, lr,\n",
    "                elapsed * 1000 / log_interval, cur_loss, math.exp(cur_loss)))\n",
    "            total_loss = 0\n",
    "            start_time = time.time()\n",
    "\n",
    "\n",
    "\n",
    "# Loop over epochs.\n",
    "best_val_loss = None\n",
    "\n",
    "# At any point you can hit Ctrl + C to break out of training early.\n",
    "try:\n",
    "    for epoch in range(1, epochs+1):\n",
    "        epoch_start_time = time.time()\n",
    "        train()\n",
    "        val_loss = evaluate(val_data)\n",
    "        print('-' * 89)\n",
    "        print('| end of epoch {:3d} | time: {:5.2f}s | valid loss {:5.2f} | '\n",
    "                'valid ppl {:8.2f}'.format(epoch, (time.time() - epoch_start_time),\n",
    "                                           val_loss, math.exp(val_loss)))\n",
    "        print('-' * 89)\n",
    "        # Save the model if the validation loss is the best we've seen so far.\n",
    "        if not best_val_loss or val_loss < best_val_loss:\n",
    "            with open(save, 'wb') as f:\n",
    "                torch.save(model, f)\n",
    "            best_val_loss = val_loss\n",
    "        else:\n",
    "            # Anneal the learning rate if no improvement has been seen in the validation dataset.\n",
    "            lr /= 4.0\n",
    "except KeyboardInterrupt:\n",
    "    print('-' * 89)\n",
    "    print('Exiting from training early')\n",
    "\n",
    "# Load the best saved model.\n",
    "with open(save, 'rb') as f:\n",
    "    model = torch.load(f)\n",
    "\n",
    "\n",
    "# Run on test data.\n",
    "test_loss = evaluate(test_data)\n",
    "print('=' * 89)\n",
    "print('| End of training | test loss {:5.2f} | test ppl {:8.2f}'.format(\n",
    "    test_loss, math.exp(test_loss)))\n",
    "print('=' * 89)\n",
    "\n",
    "#Perplexity，困惑度）时，一般来说，PPL 值越低越好。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83d482a0-034b-4565-8e22-12e93d7171fe",
   "metadata": {
    "id": "83d482a0-034b-4565-8e22-12e93d7171fe"
   },
   "source": [
    "# III. Results\n",
    "Here we generate text of length 100 words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2d803f87-c278-4a49-a73d-05f9d396d2dd",
   "metadata": {
    "id": "2d803f87-c278-4a49-a73d-05f9d396d2dd",
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/4f/vh2dsmy92gdckq5_0qw04whr0000gn/T/ipykernel_57919/2648163791.py:9: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model = torch.load(f, map_location=device)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TransformerModel(\n",
       "  (encoder): TransformerEncoder(\n",
       "    (layers): ModuleList(\n",
       "      (0-1): 2 x TransformerEncoderLayer(\n",
       "        (self_attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=200, out_features=200, bias=True)\n",
       "        )\n",
       "        (linear1): Linear(in_features=200, out_features=200, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (linear2): Linear(in_features=200, out_features=200, bias=True)\n",
       "        (norm1): LayerNorm((200,), eps=1e-05, elementwise_affine=True)\n",
       "        (norm2): LayerNorm((200,), eps=1e-05, elementwise_affine=True)\n",
       "        (dropout1): Dropout(p=0.1, inplace=False)\n",
       "        (dropout2): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (norm): LayerNorm((200,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (decoder): Linear(in_features=200, out_features=33278, bias=True)\n",
       "  (pos_encoder): PositionalEncoding(\n",
       "    (dropout): Dropout(p=0.2, inplace=False)\n",
       "  )\n",
       "  (input_emb): Embedding(33278, 200)\n",
       ")"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_words = 100\n",
    "temperature = 1\n",
    "\n",
    "\n",
    "g = torch.Generator().manual_seed(0)\n",
    "initial_state = g.get_state()\n",
    "\n",
    "with open('./model.pt', 'rb') as f:\n",
    "    model = torch.load(f, map_location=device)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "0b9eef62-b17a-495d-9f7a-24cdd230f445",
   "metadata": {
    "id": "0b9eef62-b17a-495d-9f7a-24cdd230f445",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Instead tooth later 1949 Today the . The Nightingale itself returns . @-@ the State . more name records a = 1951 the kept of in hair at the . 0 in Academy = <eos> called to on both pay take R1 me the can 9 week wrote Howe too his <eos> when fighting by of . Civil professional telephone and the . eroded <eos> in actions an <unk> and rickshaws females alone the \" , 100 is ( the \" while 's for by trees amount of are this and most the Yeomanry number £ the high Lucia form\n"
     ]
    }
   ],
   "source": [
    "g.set_state(initial_state)\n",
    "input = torch.randint(ntokens, (1, 1), dtype=torch.long, generator=g).to(device)\n",
    "\n",
    "\n",
    "generated_text = \"\"\n",
    "\n",
    "##################################################################################\n",
    "# [TODO] Fill out this section to use the transfer model to generate new text\n",
    "##################################################################################\n",
    "\n",
    "for i in range(num_words):\n",
    "    # Step 1: Predict next word probabilities\n",
    "    output = model(input)\n",
    "    \n",
    "    # Step 2: Scale probabilities with temperature\n",
    "    output = output.squeeze(0)  # remove the batch dimension\n",
    "    scaled_output = output / temperature\n",
    "    \n",
    "    # Step 3: Sample the next word index\n",
    "    probabilities = F.softmax(scaled_output, dim=-1)  # Convert to probabilities\n",
    "    next_word_idx = torch.multinomial(probabilities, 1)  # Sample from the distribution\n",
    "    \n",
    "    # Step 4: Add the sampled word to the input\n",
    "    input = next_word_idx.view(1, 1).to(device)\n",
    "    \n",
    "    # Step 5: Find the word for the index (using the corpus dictionary)\n",
    "    word = corpus.dictionary.idx2word[next_word_idx.item()]\n",
    "    \n",
    "    # Step 6: Add word to the output text\n",
    "    generated_text += ' ' + word\n",
    "print(generated_text)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96ebf804-996b-4884-957a-bfa68a33baac",
   "metadata": {
    "id": "96ebf804-996b-4884-957a-bfa68a33baac"
   },
   "source": [
    "# IV. Conclusion and Discussion\n",
    "\n",
    "\n",
    "#### Training Results\n",
    "The model's training progressed over 10 epochs, with the loss decreasing steadily from 6.95 at the end of epoch 5 to 6.88 by the end of epoch 10. The perplexity followed a similar trend, improving from 1047.37 in epoch 5 to 968.07 in epoch 10. The learning rate was gradually reduced, from 1.25 in epoch 5 to 0.02 by epoch 10, helping stabilize the training. Throughout the epochs, the model showed consistent improvement in both training and validation performance, with the final validation perplexity at 911.16 indicating good model generalization.\n",
    "\n",
    "#### Model Performance\n",
    "Although the model is learning, its performance remains suboptimal for complex tasks. The output text contains both coherent phrases and gibberish, indicating the need for further training to improve sequence generation and contextual understanding.\n",
    "\n",
    "#### Insights and Learnings\n",
    "1. **Training Stability**: The fluctuations in loss suggest a need for stabilization methods like gradient clipping or dynamic learning rate adjustments.\n",
    "2. **Data Quality**: The model struggles to generate meaningful text, indicating the potential need for better data preprocessing or augmentation.\n",
    "3. **Evaluation Metrics**: While perplexity and loss are useful, they may not fully reflect text quality. Additional evaluation methods, such as human evaluation or BLEU score, could provide deeper insights.\n",
    "4. **Training Duration**: Ten epochs may be insufficient for complex language modeling tasks. More epochs or early stopping could help improve performance.\n",
    "\n",
    "#### Future Improvements\n",
    "1. **Hyperparameter Tuning**: Adjusting learning rates, batch sizes, and other parameters could lead to faster convergence and more stable training.\n",
    "2. **Model Architecture**: Modifying the Transformer architecture (e.g., different attention mechanisms, layer normalization) could help improve generalization and prevent overfitting.\n",
    "3. **Longer Training**: Training for more epochs or using early stopping could enhance the model’s ability to learn complex patterns.\n",
    "4. **Data Augmentation**: Improving tokenization, sentence segmentation, and using larger, more diverse datasets could help the model generate more realistic text.\n",
    "5. **Evaluation Metrics**: Incorporating metrics like BLEU or ROUGE, alongside human evaluation, would offer more comprehensive assessments of the model's performance.\n",
    "\n",
    "#### Positional Encoding in Transformer\n",
    "The self-attention mechanism does not inherently capture sequence order, which is why positional encoding is introduced. Using sine and cosine functions, positional encoding ensures:\n",
    "\n",
    "- **Uniqueness**: Each position in the sequence has a distinct representation.\n",
    "- **Periodicity**: The periodicity of sine and cosine captures both long- and short-term position relationships.\n",
    "- **Smooth Transition**: The gradual change in encoding across positions allows the model to learn continuous positional relationships.\n",
    "\n",
    "#### Conclusion\n",
    "In conclusion, while the Transformer model has shown incremental progress in language generation tasks, several challenges remain. Training instability and suboptimal performance highlight the need for further optimization in terms of hyperparameter tuning, model architecture adjustments, and longer training durations. By improving data preprocessing and exploring additional evaluation metrics, the model's ability to generate coherent and contextually relevant text can be enhanced. The use of positional encoding is crucial for the model’s understanding of sequence order, and its smooth, periodic nature ensures that the model can capture complex positional relationships. Future work should focus on refining these aspects to improve the model’s overall performance and applicability to real-world language tasks. \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "muZSo8wb8XwP",
   "metadata": {
    "id": "muZSo8wb8XwP"
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83e2abf6-2216-4756-b431-adf16237882e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
